# ğŸ§¬ VirTransformer-250bp

**VirTransformer-250bp** is a Transformer-based AI model designed to predict whether a viral genome can infect *Homo sapiens* cells directly from **nucleotide sequences** â€” without using host or protein biomarkers.



![Model Architecture](model.png)

**VirTransformer-250bp** is a Transformer-based AI model designed to predict...

This repository includes:
- ğŸ§  **Hugging Face implementation** â€” main app, model, and interface
- âš™ï¸ **Python client** â€” for local or Render deployment
- ğŸ–¼ï¸ **Model architecture** â€” visualized in `model.png`

---

## ğŸš€ Overview

VirTransformer-250bp analyzes raw DNA sequences (usually 250 base pairs) and predicts:
- âœ… Potential to infect human cells
- This model can be Finetune for numerous downstream Tasks
   ğŸ§« ICTV taxonomic classification  
   ğŸ§¬ Baltimore classification  
   â˜£ï¸ Oncogenic potential  

The model was trained on thousands of diverse viral genomes and evaluated on unseen species, achieving over **92.5% accuracy**.

---

## ğŸ“‚ Repository Structure

```
VirTransformer-250bp/
â”‚
â”œâ”€â”€ HuggingFace/           # Hugging Face Space app and model
â”‚   â”œâ”€â”€ app.py             # Gradio interface for user input & prediction
â”‚   â”œâ”€â”€ requirements.txt   # Space dependencies
â”‚   â””â”€â”€ model/             # Trained transformer weights (if included)
â”‚
â”œâ”€â”€ Client/                # Python client for local/Render deployment
â”‚   â”œâ”€â”€ client.py          # Script for local predictions
â”‚   â”œâ”€â”€ api.py             # API backend (optional)
â”‚   â””â”€â”€ requirements.txt   # Client dependencies
â”‚
â”œâ”€â”€ model.png              # Model architecture / workflow diagram
â””â”€â”€ README.md              # Documentation
```

---

## ğŸ§© Features

- Accepts raw viral nucleotide or FASTA sequences  
- Predicts multiple viral properties using a single transformer model  
- Interactive Gradio app on Hugging Face  
- Easily deployable via **Render** or **local environment**  
- Built with **PyTorch**, **Transformers**, and **Gradio**

---

## âš™ï¸ How It Works

1. **Input:** A viral nucleotide sequence (â‰ˆ250bp)  
2. **Tokenization:** Converts sequence into overlapping k-mers  
3. **Embedding:** Passes through transformer encoder layers  
4. **Prediction:** Outputs probabilities for infection and classification  
5. **Visualization:** Gradio app displays results interactively  

---

## ğŸ§  Model Deployment

### â–¶ï¸ On Hugging Face

Try it online:  
ğŸ”— [https://rajaatif786-virtransformer-250bp.hf.space](https://rajaatif786-virtransformer-250bp.hf.space)

Paste a nucleotide sequence and get:
- Predicted human infectivity  
- Virus class and other properties  

### ğŸ Local Prediction (Python Client)

```bash
cd Client
pip install -r requirements.txt
python client.py
```

Then enter your viral sequence when prompted.

---

## ğŸ“Š Model Details

- **Architecture:** Transformer Encoder  
- **Input length:** 250bp nucleotide fragments  
- **Training:** Viral genome datasets (species-level and virus-level split)  
- **Accuracy:** >92.5%  
- **Hardware:** NVIDIA A100 GPU  
- **Frameworks:** PyTorch + Gradio  

---

## ğŸ“œ Citation

If you use this project in your research, please cite:

> **Raja Atif Aurang Zaib.** (2024).  
> *Prediction of Human Pathogenicity from Viral Genome Sequences Using Transformer-Based Deep Learning.*  
> SSRN Electronic Journal.  
> [https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4978320](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4978320)

---

## ğŸ’¡ Acknowledgment

Inspired by the work of **Jakub Bartoszewicz** on interpretable viral detection.  
Developed and extended independently by **Raja Atif Aurang Zaib**.

---

## ğŸ§° License

Released under the **MIT License**.  
Youâ€™re free to use, modify, and distribute with proper attribution.

---

## ğŸ“¬ Contact

**Author:** Raja Atif Aurang Zaib  
ğŸŒ Hugging Face: [@rajaatif786](https://huggingface.co/rajaatif786)  
ğŸ”— Demo: [VirTransformer-250bp App](https://bioaml.com/pathogenicity.html)
